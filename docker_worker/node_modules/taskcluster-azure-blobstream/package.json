{
  "name": "taskcluster-azure-blobstream",
  "version": "0.0.1",
  "description": "taskcluster-azure-blobstream ==================",
  "main": "block_stream.js",
  "scripts": {
    "test": "make test"
  },
  "author": "",
  "license": "BSD-2-Clause",
  "dependencies": {
    "azure": "0.7.19",
    "promise": "~3.2.0"
  },
  "devDependencies": {
    "mocha": "~1.17.0",
    "uuid": "~1.4.1",
    "mocha-as-promised": "~2.0.0"
  },
  "readme": "taskcluster-azure-blobstream\n==================\n\nStream interface built on top of azure for incrementally pushing buffers\nand committing them. Designed for \"live\" logging and bursty streams of\ndata (which is expected to end eventually).\n\nThere are many common cases which the azure client handles _much_ better\nthen this library if you doing any of the following use the azure\nclient:\n\n  - writing never ending data (that may be rolled over)\n  - randomly accessing or updating blocks/pages\n  - uploading files already on disk\n  - uploading a stream indefinitely\n\n## Strategy\n\nThe algorithm is very simple (dumb)\n\n - let node stream handle buffering/backpressure\n - write block (BlobBlock) and commit it in same write operation (_write in node streams)\n - now that block is readable\n \nDue to how node streams work while we are writing the readable side will buffer its writes up to the high water mark.\n\n## Example\n\n```js\nvar AzureStream = require('taskcluster-azure-blobstream');\n\nvar azure = require('azure');\nvar blobService = azure.createBlobService();\n\nvar azureWriter = new AzureStream(\n  blobService,\n  'mycontainer',\n  'myfile.txt'\n);\n\n// any kind of node readable stream here\nvar nodeStream;\n\nnodeStream.pipe(azureWriter);\nazureWriter.once('finish', function() {\n  // yey data was written\n  // get the url\n  console.log(blobService.getBlobUrl('mycontainer', 'myfile.txt'));\n});\n\n```\n\n## RANDOM NOTES\n\nthe `azure` module is very slow to load (330ms) and takes up 33mb of\nmemory (as of 0.7.19). We don't use very many azure blob api calls so\nideally we could extract (or help the primary lib extract) the url\nsigning part of authentication into its own lib and then just directly\ncall http for our operations... The ultimate goal here is to consume\naround 5mb (including https overhead) of memory and load in under 20ms.\n\nTo correctly consume the url from azure the `x-ms-version` header must\nbe set to something like `2013-08-15` this allows open ended range\nrequests (`range: byte=500-`). In combination with etags (and if\nconditions) we can build a very fast client (even a fast polling\nclient).\n",
  "readmeFilename": "README.md",
  "_id": "taskcluster-azure-blobstream@0.0.1",
  "dist": {
    "shasum": "6d8a37b7893a192f0f6a535b99c7f2e186730cfc"
  },
  "_from": "taskcluster-azure-blobstream@0.0.1",
  "_resolved": "https://registry.npmjs.org/taskcluster-azure-blobstream/-/taskcluster-azure-blobstream-0.0.1.tgz"
}
